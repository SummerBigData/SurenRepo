Using TensorFlow backend.
/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
You have chosen: Namespace(bsize=100, epo=70, f1=11250, f2=50, f3=100, f4=1, h=0, imgsize=67, m=1604, trimsize=8)
 
x and y (4000, 67, 67, 3) (4000,)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 65, 65, 64)        1792      
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 32, 32, 64)        0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 32, 32, 64)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 30, 30, 128)       73856     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 15, 15, 128)       0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 15, 15, 128)       0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 13, 13, 128)       147584    
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 6, 6, 128)         0         
_________________________________________________________________
dropout_3 (Dropout)          (None, 6, 6, 128)         0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 4, 4, 64)          73792     
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 2, 2, 64)          0         
_________________________________________________________________
dropout_4 (Dropout)          (None, 2, 2, 64)          0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 256)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               131584    
_________________________________________________________________
activation_1 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_5 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 256)               131328    
_________________________________________________________________
activation_2 (Activation)    (None, 256)               0         
_________________________________________________________________
dropout_6 (Dropout)          (None, 256)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 1)                 257       
_________________________________________________________________
activation_3 (Activation)    (None, 1)                 0         
=================================================================
Total params: 560,193
Trainable params: 560,193
Non-trainable params: 0
_________________________________________________________________
Train on 4000 samples, validate on 604 samples
Epoch 1/70
2018-07-05 15:47:32.356178: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-07-05 15:47:32.356213: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
218s - loss: 0.8205 - acc: 0.5517 - val_loss: 0.5886 - val_acc: 0.5944
Epoch 2/70
218s - loss: 0.5618 - acc: 0.6928 - val_loss: 0.5289 - val_acc: 0.6639
Epoch 3/70
216s - loss: 0.5255 - acc: 0.7380 - val_loss: 0.4967 - val_acc: 0.7152
Epoch 4/70
217s - loss: 0.4373 - acc: 0.7980 - val_loss: 0.4054 - val_acc: 0.7980
Epoch 5/70
217s - loss: 0.3980 - acc: 0.8170 - val_loss: 0.3778 - val_acc: 0.8129
Epoch 6/70
217s - loss: 0.3659 - acc: 0.8407 - val_loss: 0.3660 - val_acc: 0.8146
Epoch 7/70
216s - loss: 0.3313 - acc: 0.8513 - val_loss: 0.3310 - val_acc: 0.8328
Epoch 8/70
216s - loss: 0.2979 - acc: 0.8610 - val_loss: 0.3097 - val_acc: 0.8427
Epoch 9/70
218s - loss: 0.2833 - acc: 0.8770 - val_loss: 0.3030 - val_acc: 0.8560
Epoch 10/70
217s - loss: 0.2761 - acc: 0.8715 - val_loss: 0.3324 - val_acc: 0.8394
Epoch 11/70
217s - loss: 0.2554 - acc: 0.8815 - val_loss: 0.2785 - val_acc: 0.8659
Epoch 12/70
217s - loss: 0.2382 - acc: 0.8925 - val_loss: 0.3036 - val_acc: 0.8609
Epoch 13/70
217s - loss: 0.2330 - acc: 0.8957 - val_loss: 0.3137 - val_acc: 0.8593
Epoch 14/70
222s - loss: 0.2315 - acc: 0.8980 - val_loss: 0.3108 - val_acc: 0.8609
Epoch 15/70
